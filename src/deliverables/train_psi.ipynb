{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Coarse-Grid Operator model and create an MGRIT solver\n",
    "\n",
    "***Date:** August 19th, 2024*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Dagrazs-MacBook-Air.local:52121] shmem: mmap: an error occurred while determining whether or not /var/folders/fd/8p6y3p9d67l9ms3hzwcf97vm0000gn/T//ompi.Dagrazs-MacBook-Air.501/jf.0/3518955520/sm_segment.Dagrazs-MacBook-Air.501.d1bf0000.0 could be created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10efd9150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ConstantLR, ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse import identity\n",
    "\n",
    "from pymgrit.core.application import Application\n",
    "from pymgrit.heat.heat_1d import VectorHeat1D\n",
    "from pymgrit.core.mgrit import Mgrit\n",
    "\n",
    "from loss_functions import NonGalerkinLoss1Rand, NonGalerkinLoss1Eig, NonGalerkinLoss2Eig, NonGalerkinLoss2EigConst, NonGalerkinLoss3Eig, NonGalerkinLoss3EigConst\n",
    "from constants import (\n",
    "    device,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    hidden_size,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    nstencils,\n",
    "    eps,\n",
    "    plot_landscape,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, array):\n",
    "        y = self.linear_relu_stack(array)\n",
    "        return y\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonGalerkinDataset(Dataset):\n",
    "    def __init__(self, array, transform=None, target_transform=None):\n",
    "        self.stencil = array\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.stencil)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        out = self.stencil[index]\n",
    "        if self.transform:\n",
    "            out = self.transform(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses one stencil and 32 coarsening factors\n",
    "# Augment this to use multiple stencils (different beta's)\n",
    "dx = 1 / 16\n",
    "dt = 1 / 4096\n",
    "beta = dt / dx ** 2\n",
    "stencil_dataset = torch.Tensor([beta, 1 - 2 * beta, beta]).repeat(nstencils, 1)\n",
    "m = torch.arange(1, nstencils + 1).unsqueeze(1)\n",
    "stencil_dataset = torch.cat((stencil_dataset, m), 1)\n",
    "\n",
    "# # Set up for using different stencils\n",
    "# nstencils = 4\n",
    "# betas = torch.tensor([1/8, 1/12, 1/16, 1/24])\n",
    "# stencil_datasets = torch.torch.empty((0, 4), dtype=torch.float32)\n",
    "# for beta in betas:\n",
    "#     stencil_dataset = torch.Tensor([beta, 1 - 2 * beta, beta]).repeat(nstencils, 1)\n",
    "#     m = torch.arange(1, nstencils + 1).unsqueeze(1)\n",
    "#     stencil_dataset = torch.cat((stencil_dataset, m), 1)\n",
    "#     stencil_datasets = torch.cat((stencil_datasets, stencil_dataset), 0)\n",
    "\n",
    "stencil_dataset = stencil_dataset.to(device)\n",
    "\n",
    "dataset = NonGalerkinDataset(stencil_dataset)\n",
    "Ndata = len(dataset)\n",
    "Ntest = 0\n",
    "Ntrain = Ndata - Ntest\n",
    "dataset_train, dataset_test = random_split(dataset, [Ntrain, Ntest])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# print test dataset\n",
    "print(f\"Train set size {Ntrain}, Test set size {Ntest}, batch size {batch_size}\")\n",
    "print(stencil_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, eps=1e-2):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        input_batch = X\n",
    "        output_batch = model(X)\n",
    "\n",
    "        # Modify inputs to loss function depending on loss function used\n",
    "        loss = loss_fn(input_batch, output_batch)\n",
    "        \n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if batch % max(1, num_batches - 1) == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(input_batch)\n",
    "            print(f\"Train Loss: {loss:>7e}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = NonGalerkinLoss1Rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "# scheduler = optim.lr_scheduler.ConstantLR(optimizer, learn_rate_drop_factor)\n",
    "\n",
    "loss_vis = np.zeros((epochs, nstencils // batch_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_loader, net, loss_fn, optimizer)\n",
    "    # Print change of loss throughout training\n",
    "    loss_vis[epoch] = np.array(loss)\n",
    "    \n",
    "    # # Modify eps throughout training for Loss 1\n",
    "    # if ((epoch + 1) % eps_drop_period == 0):\n",
    "    #     eps = eps / 2\n",
    "\n",
    "    # # Modify learning rate throughout training\n",
    "    # if ((epoch + 1) % learn_rate_drop_period == 0):\n",
    "    #     scheduler.step()\n",
    "    #     print(\"Update learning rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_coord = (nstencils // batch_size) * epochs\n",
    "loss_vis = np.reshape(loss_vis, (1, num_of_coord))\n",
    "x = np.linspace(0, num_of_coord, num_of_coord)\n",
    "plt.plot(x, loss_vis[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load the Trained NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"psi-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"psi-model\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the phi we trained on\")\n",
    "dt = 1 / 4096\n",
    "dx = 1/ 16\n",
    "beta = dt / dx ** 2\n",
    "m = 4\n",
    "phi_m = torch.tensor([beta, 1 - 2 * beta, beta, m]).to(device).reshape((1,4))\n",
    "psi = model(phi_m).reshape((1,3))\n",
    "disc = torch.tensor([m*beta, 1 - 2 * m*beta, m*beta]).to(device).reshape((1,3))\n",
    "loss_psi = loss_fn(phi_m, psi, eps)\n",
    "loss_disc = loss_fn(phi_m, disc, eps)\n",
    "print(f\"phi = {phi_m}\")\n",
    "print(f\"psi = {psi.detach()}\")\n",
    "print(f\"output of loss function on psi: {loss_psi}\")\n",
    "print(f\"dsc = {disc}\")\n",
    "print(f\"output of loss function on disc: {loss_disc}\")\n",
    "\n",
    "print()\n",
    "print(\"This is not the phi we trained on\")\n",
    "dt = 1 / 2560\n",
    "dx = 1/ 16\n",
    "beta = dt / dx ** 2\n",
    "m = 2\n",
    "phi_m = torch.tensor([beta, 1 - 2 * beta, beta, m]).to(device).reshape((1,4))\n",
    "psi = model(phi_m).reshape((1,3))\n",
    "disc = torch.tensor([m*beta, 1 - 2 * m*beta, m*beta]).to(device).reshape((1,3))\n",
    "loss_psi = loss_fn(phi_m, psi, eps)\n",
    "loss_disc = loss_fn(phi_m, disc, eps)\n",
    "print(f\"phi = {phi_m}\")\n",
    "print(f\"psi = {psi.detach()}\")\n",
    "print(f\"output of loss function on psi: {loss_psi}\")\n",
    "print(f\"dsc = {disc}\")\n",
    "print(f\"output of loss function on disc: {loss_disc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Optiomal Stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1 / 4096\n",
    "dx = 1/ 16\n",
    "beta = dt / dx ** 2\n",
    "m = 4\n",
    "phi_m = torch.tensor([beta, 1 - 2 * beta, beta, m]).to(device)\n",
    "print(phi_m)\n",
    "\n",
    "x0 = torch.tensor([x[0], x[1], x[0]]).reshape((1,3))\n",
    "x0[0,-1]\n",
    "\n",
    "print(x0[0,1:])\n",
    "\n",
    "def loss(x):\n",
    "    loss = loss_fn(phi_m.reshape((1,4)), torch.tensor([x[0], x[1], x[0]]).reshape((1,3)), eps)\n",
    "    return loss.item()\n",
    "\n",
    "opt = scipy.optimize.minimize(loss, [1,1], method='Nelder-Mead')\n",
    "optimal_stencil = torch.tensor([opt.x[0], opt.x[1], opt.x[0]])\n",
    "print(optimal_stencil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Landscape\n",
    "if plot_landscape:\n",
    "    n = 200\n",
    "    dt = 1/4096\n",
    "    dx = 1/16\n",
    "    beta = dt / dx ** 2\n",
    "    m = 4\n",
    "    phi_m = torch.tensor([beta, 1 - 2 *  beta, beta, m]).to(device).reshape([1, 4])\n",
    "    psis = torch.from_numpy(np.fromfunction(\n",
    "        lambda i, j: np.array([i / n, j / n, i / n]),\n",
    "        [n, n]\n",
    "    )).to(device)\n",
    "\n",
    "    losses = torch.tensor(size = [n, n])\n",
    "    for i in range(n):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        for j in range(n):\n",
    "            losses[i, j] = loss_fn(phi_m, psis[:, i, j].reshape([1, 3]), eps)\n",
    "\n",
    "                \n",
    "    plt.imshow(losses, aspect = 1, origin = 'lower', norm=LogNorm(vmin=0.001, vmax=1))\n",
    "    plt.colorbar()\n",
    "    plt.scatter([100], [50], c = 'red', marker = 'x')\n",
    "    plt.xticks(range(0, n + 1, n / 5), [0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.yticks(range(0, n + 1, n / 5), [0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title('Loss 1 with rand and const, eps=1e-2, for matrix [a, b, a]')\n",
    "    plt.xlabel('b')\n",
    "    plt.ylabel('a')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMGRIT Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector and application class for the 1D heat equation\n",
    "\n",
    "This extends the Python code\n",
    "\"\"\"\n",
    "##########\n",
    "\n",
    "class Heat1DExp(Application):\n",
    "    \"\"\"\n",
    "    Application class for the heat equation in 1D space,\n",
    "        u_t - a*u_xx = b(x,t),  a > 0, x in [x_start,x_end], t in [0,T],\n",
    "    with periodic boundary conditions in space and explicit forward Euler discretization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_start, x_end, nx, a, init_cond=lambda x: x * 0, rhs=lambda x, t: x * 0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        :param x_start: left interval bound of spatial domain\n",
    "        :param x_end: right interval bound of spatial domain\n",
    "        :param nx: number of spatial degrees of freedom\n",
    "        :param a: thermal conductivity\n",
    "        :param init_cond: initial condition\n",
    "        :param rhs: right-hand side\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Spatial domain with homogeneous Dirichlet boundary conditions\n",
    "        self.x_start = x_start\n",
    "        self.x_end = x_end\n",
    "        self.x = np.linspace(self.x_start, self.x_end, nx)\n",
    "        self.x = self.x[0:-1]\n",
    "        self.nx = nx - 1\n",
    "        self.dx = self.x[1] - self.x[0]\n",
    "\n",
    "        # Thermal conductivity\n",
    "        self.a = a\n",
    "\n",
    "        # Set (spatial) identity matrix and spatial discretization matrix\n",
    "        self.identity = identity(self.nx, dtype='float', format='csr')\n",
    "        self.space_disc = self.compute_matrix()\n",
    "\n",
    "        # Set right-hand side routine\n",
    "        self.rhs = rhs\n",
    "\n",
    "        # Set the data structure for any user-defined time point\n",
    "        self.vector_template = VectorHeat1D(self.nx)\n",
    "\n",
    "        # Set initial condition\n",
    "        self.init_cond = init_cond\n",
    "        self.vector_t_start = VectorHeat1D(self.nx)\n",
    "        self.vector_t_start.set_values(self.init_cond(self.x))\n",
    "\n",
    "    def compute_matrix(self):\n",
    "        \"\"\"\n",
    "        Define spatial discretization matrix for 1D heat equation (forward Euler)\n",
    "\n",
    "        Second-order central finite differences with matrix stencil (periodic BCs)\n",
    "           (a / dx^2) * [-1  2  -1]\n",
    "        \"\"\"\n",
    "\n",
    "        fac = self.a / self.dx ** 2\n",
    "\n",
    "        diagonal = np.ones(self.nx) * 2 * fac\n",
    "        lower = np.ones(self.nx - 1) * -fac\n",
    "        upper = np.ones(self.nx - 1) * -fac\n",
    "\n",
    "        # diagonal = np.ones(self.nx) * 2 * 0.6231\n",
    "        # lower = np.ones(self.nx - 1) * -0.1721\n",
    "        # upper = np.ones(self.nx - 1) * -0.1715\n",
    "\n",
    "        matrix = sp.diags(\n",
    "            diagonals=[diagonal, lower, upper, lower, upper],\n",
    "            offsets=[0, -1, 1, self.nx-1, -self.nx+1], shape=(self.nx, self.nx),\n",
    "            format='csr')\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def step(self, u_start: VectorHeat1D, t_start: float, t_stop: float) -> VectorHeat1D:\n",
    "        \"\"\"\n",
    "        Time integration routine for 1D heat equation example problem:\n",
    "            Forward Euler\n",
    "\n",
    "        One-step method\n",
    "           u_i = (I - dt*L) * u_{i-1} + dt*b_{i-1},\n",
    "        where L = self.space_disc is the spatial discretization operator\n",
    "\n",
    "        :param u_start: approximate solution for the input time t_start\n",
    "        :param t_start: time associated with the input approximate solution u_start\n",
    "        :param t_stop: time to evolve the input approximate solution to\n",
    "        :return: approximate solution at input time t_stop\n",
    "        \"\"\"\n",
    "        dt = (t_stop - t_start)\n",
    "        tmp = u_start.get_values()\n",
    "        # This is the implicit step for reference\n",
    "        # tmp = spsolve(self.identity + dt * self.space_disc,\n",
    "        #               tmp + dt * self.rhs(self.x, t_stop))\n",
    "        tmp = (self.identity - dt * self.space_disc) * tmp + dt * self.rhs(self.x, t_start)\n",
    "        ret = VectorHeat1D(len(tmp))\n",
    "        ret.set_values(tmp)\n",
    "        return ret\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "class Heat1DNN(Application):\n",
    "    \"\"\"\n",
    "    Application class for the heat equation in 1D space,\n",
    "        u_t - a*u_xx = b(x,t),  a > 0, x in [x_start,x_end], t in [0,T],\n",
    "    with periodic boundary conditions in space and offline-trained neural network model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_start, x_end, nx, a, m, init_cond=lambda x: x * 0, rhs=lambda x, t: x * 0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        :param x_start: left interval bound of spatial domain\n",
    "        :param x_end: right interval bound of spatial domain\n",
    "        :param nx: number of spatial degrees of freedom\n",
    "        :param a: thermal conductivity\n",
    "        :param init_cond: initial condition\n",
    "        :param rhs: right-hand side\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Spatial domain with homogeneous Dirichlet boundary conditions\n",
    "        self.x_start = x_start\n",
    "        self.x_end = x_end\n",
    "        self.x = np.linspace(self.x_start, self.x_end, nx)\n",
    "        self.x = self.x[0:-1]\n",
    "        self.nx = nx - 1\n",
    "        self.dx = self.x[1] - self.x[0]\n",
    "        self.m = m\n",
    "\n",
    "        # Thermal conductivity\n",
    "        self.a = a\n",
    "\n",
    "        # Set right-hand side routine\n",
    "        self.rhs = rhs\n",
    "\n",
    "        # Set the data structure for any user-defined time point\n",
    "        self.vector_template = VectorHeat1D(self.nx)\n",
    "\n",
    "        # Set initial condition\n",
    "        self.init_cond = init_cond\n",
    "        self.vector_t_start = VectorHeat1D(self.nx)\n",
    "        self.vector_t_start.set_values(self.init_cond(self.x))\n",
    "\n",
    "    def step(self, u_start: VectorHeat1D, t_start: float, t_stop: float) -> VectorHeat1D:\n",
    "        \"\"\"\n",
    "        Time integration routine for 1D heat equation example problem:\n",
    "            Neural Network Model\n",
    "\n",
    "        One-step method\n",
    "           u_i = psi * u_{i-1} + dt*b_{i-1},\n",
    "        where psi is the NN model\n",
    "\n",
    "        :param u_start: approximate solution for the input time t_start\n",
    "        :param t_start: time associated with the input approximate solution u_start\n",
    "        :param t_stop: time to evolve the input approximate solution to\n",
    "        :return: approximate solution at input time t_stop\n",
    "        \"\"\"\n",
    "\n",
    "        # compute dt and beta for the finest mesh\n",
    "        dt = (t_stop - t_start) / self.m\n",
    "        beta = dt * self.a / self.dx ** 2\n",
    "\n",
    "        # use NN to get psi\n",
    "        phi_m = torch.Tensor([beta, 1 - 2 * beta, beta, self.m]).to(device)\n",
    "        psi_model = model(phi_m)\n",
    "\n",
    "        # this assumes a 3-point stencil (for now)\n",
    "        diagonal = np.ones(self.nx)  * psi_model[1].item()\n",
    "        lower = np.ones(self.nx - 1) * psi_model[0].item()\n",
    "        upper = np.ones(self.nx - 1) * psi_model[2].item()\n",
    "\n",
    "        psi = sp.diags(\n",
    "            diagonals=[diagonal, lower, upper, lower, upper],\n",
    "            offsets=[0, -1, 1, self.nx-1, -self.nx+1], shape=(self.nx, self.nx),\n",
    "            format='csr')\n",
    "        \n",
    "        tmp = u_start.get_values()\n",
    "        tmp = psi * tmp + dt * self.rhs(self.x, t_start)\n",
    "        ret = VectorHeat1D(len(tmp))\n",
    "        ret.set_values(tmp)\n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply two-level MGRIT V-cycles with FCF-relaxation to solve the 1D heat equation\n",
    "    u_t - a*u_xx = b(x,t),  a > 0, x in [0,1], t in [0,T],\n",
    "with RHS b(x,t) = 0,\n",
    "periodic BCs in space,\n",
    "    u(0,t)   = u(1,t),    t in [0,T],\n",
    "    u_x(0,t) = u_x(1,t),  t in [0,T],\n",
    "and subject to the initial condition\n",
    "    u(x,0)  = sin^2(pi*x),  x in [0,1]\n",
    "\n",
    "=> exact solution u(x,t) = sin^2(pi*x)*cos(t)\n",
    "\"\"\"\n",
    "\n",
    "def main(plotting=False):\n",
    "    def rhs(x, t):\n",
    "        \"\"\"\n",
    "        Right-hand side of 1D heat equation example problem at a given space-time point (x,t),\n",
    "        :param x: spatial grid point\n",
    "        :param t: time point\n",
    "        :return: right-hand side of 1D heat equation example problem at point (x,t)\n",
    "        \"\"\"\n",
    "\n",
    "        return -(np.sin(t))*(np.sin(np.pi*x)**2) - 2*(np.pi**2)*(np.cos(t))*((np.cos(np.pi*x)**2) - (np.sin(np.pi*x)**2))\n",
    "\n",
    "    def init_cond(x):\n",
    "        \"\"\"\n",
    "        Initial condition of 1D heat equation example,\n",
    "        :param x: spatial grid point\n",
    "        :return: initial condition of 1D heat equation example problem\n",
    "        \"\"\"\n",
    "        return np.sin(np.pi * x) ** 2\n",
    "\n",
    "    # Domain: space = [0,1], time = [0,1]\n",
    "    # nx = 17    =>  dx = 1/16\n",
    "    # nt = 4097  =>  dt = 1/4096\n",
    "    #            =>  beta = dt/dx^2 = 256/4096 = 1/16  (allows coarsening by up to 8)\n",
    "    # Coarsen by a factor of 4 in time (from 4097 to 1025 points)\n",
    "    # \n",
    "    heat0 = Heat1DExp(x_start=0, x_end=1, nx=17, a=1, init_cond=init_cond, rhs=rhs, t_start=0, t_stop=1, nt=2561)\n",
    "    heat1 = Heat1DNN(x_start=0, x_end=1, nx=17, a=1, m=2, init_cond=init_cond, rhs=rhs, t_start=0, t_stop=1, nt=1281)\n",
    "    heat2 = Heat1DNN(x_start=0, x_end=1, nx=17, a=1, m=4, init_cond=init_cond, rhs=rhs, t_start=0, t_stop=1, nt=641)\n",
    "\n",
    "    # Setup two-level MGRIT solver and solve the problem\n",
    "    problem = [heat0, heat1, heat2]\n",
    "    mgrit = Mgrit(problem=problem, cf_iter=1, cycle_type='F', nested_iteration=False, max_iter=10,\n",
    "                  logging_lvl=20, random_init_guess=True)\n",
    "    info = mgrit.solve()\n",
    "\n",
    "    if plotting:\n",
    "        nt = len(mgrit.u[0])\n",
    "        nx = mgrit.u[0][0].size\n",
    "        values = [mgrit.u[0][i].get_values() for i in range(nt)]\n",
    "        sol = np.vstack([np.array(val) for val in values])\n",
    "        plt.imshow(sol, aspect=nx/nt, origin='lower')\n",
    "        plt.colorbar()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(plotting=True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
